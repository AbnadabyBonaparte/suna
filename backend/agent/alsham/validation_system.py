"""
SUNA-ALSHAM Validation System
Sistema de valida√ß√£o cient√≠fica para agentes auto-evolutivos

Garante rigor cient√≠fico nas melhorias e evolu√ß√£o dos agentes
Integrado com infraestrutura SUNA existente
"""

import uuid
import json
import time
import math
from datetime import datetime, timedelta
from typing import Dict, Any, Optional, List, Tuple
import logging
import statistics

# Importa√ß√µes SUNA existentes
try:
    from ...supabase import get_supabase_client
    from ...utils.logger import get_logger
except ImportError:
    # Fallback para desenvolvimento
    logging.basicConfig(level=logging.INFO)
    get_logger = lambda name: logging.getLogger(name)

class ValidationSystem:
    """
    Sistema de valida√ß√£o cient√≠fica para agentes SUNA-ALSHAM
    
    Funcionalidades:
    - Valida√ß√£o estat√≠stica de melhorias
    - Testes de signific√¢ncia cient√≠fica
    - An√°lise de reprodutibilidade
    - Controle de qualidade cient√≠fica
    """
    
    def __init__(self, config: Optional[Dict] = None):
        self.system_id = str(uuid.uuid4())
        self.config = config or {}
        
        # Configura√ß√µes cient√≠ficas
        self.min_improvement_percentage = self.config.get('min_improvement_percentage', 20.0)
        self.significance_level = self.config.get('significance_level', 0.05)  # p-value < 0.05
        self.min_sample_size = self.config.get('min_sample_size', 5)
        self.reproducibility_threshold = self.config.get('reproducibility_threshold', 0.8)
        
        # Crit√©rios de valida√ß√£o
        self.validation_criteria = {
            "statistical_significance": True,
            "practical_significance": True,
            "reproducibility": True,
            "consistency": True,
            "bounds_check": True
        }
        
        # Cache de valida√ß√µes
        self.validation_cache = {}
        self.validation_history = []
        
        # Logger
        self.logger = get_logger("SUNA-ALSHAM-VALIDATION")
        
        # Integra√ß√£o SUNA
        self.supabase_client = None
        self._initialize_suna_integration()
        
        self.logger.info(f"üî¨ Sistema de Valida√ß√£o inicializado - ID: {self.system_id}")
    
    def _initialize_suna_integration(self):
        """Inicializa integra√ß√£o com infraestrutura SUNA"""
        try:
            self.supabase_client = get_supabase_client()
            self.logger.info("‚úÖ Integra√ß√£o SUNA inicializada com sucesso")
        except Exception as e:
            self.logger.warning(f"‚ö†Ô∏è Integra√ß√£o SUNA parcial: {e}")
    
    def validate_improvement(self, agent_id: str, improvement_data: Dict[str, Any]) -> Dict[str, Any]:
        """
        Valida melhoria de um agente usando crit√©rios cient√≠ficos rigorosos
        """
        self.logger.info(f"üî¨ Validando melhoria do agente {agent_id}...")
        
        validation_id = str(uuid.uuid4())
        validation_start = time.time()
        
        try:
            # 1. Valida√ß√£o de signific√¢ncia estat√≠stica
            statistical_validation = self._validate_statistical_significance(agent_id, improvement_data)
            
            # 2. Valida√ß√£o de signific√¢ncia pr√°tica
            practical_validation = self._validate_practical_significance(improvement_data)
            
            # 3. Valida√ß√£o de reprodutibilidade
            reproducibility_validation = self._validate_reproducibility(agent_id, improvement_data)
            
            # 4. Valida√ß√£o de consist√™ncia
            consistency_validation = self._validate_consistency(agent_id, improvement_data)
            
            # 5. Valida√ß√£o de limites
            bounds_validation = self._validate_bounds(improvement_data)
            
            # Consolidar resultados
            validation_results = {
                "statistical": statistical_validation,
                "practical": practical_validation,
                "reproducibility": reproducibility_validation,
                "consistency": consistency_validation,
                "bounds": bounds_validation
            }
            
            # Determinar aprova√ß√£o geral
            all_passed = all(result.get("passed", False) for result in validation_results.values())
            
            validation_duration = time.time() - validation_start
            
            final_validation = {
                "validation_id": validation_id,
                "agent_id": agent_id,
                "timestamp": datetime.utcnow().isoformat(),
                "duration_seconds": validation_duration,
                "improvement_data": improvement_data,
                "validation_results": validation_results,
                "overall_passed": all_passed,
                "confidence_score": self._calculate_confidence_score(validation_results),
                "recommendations": self._generate_recommendations(validation_results)
            }
            
            # Salvar valida√ß√£o
            self.validation_history.append(final_validation)
            self.validation_cache[agent_id] = final_validation
            
            # Salvar no SUNA
            self._save_validation_to_suna(final_validation)
            
            if all_passed:
                self.logger.info(f"‚úÖ Valida√ß√£o APROVADA para agente {agent_id}")
            else:
                self.logger.warning(f"‚ùå Valida√ß√£o REPROVADA para agente {agent_id}")
            
            return final_validation
            
        except Exception as e:
            self.logger.error(f"‚ùå Erro na valida√ß√£o: {e}")
            return {
                "validation_id": validation_id,
                "agent_id": agent_id,
                "error": str(e),
                "overall_passed": False,
                "timestamp": datetime.utcnow().isoformat()
            }
    
    def _validate_statistical_significance(self, agent_id: str, improvement_data: Dict[str, Any]) -> Dict[str, Any]:
        """Valida signific√¢ncia estat√≠stica da melhoria"""
        
        try:
            # Obter hist√≥rico de performance do agente
            performance_history = self._get_performance_history(agent_id)
            
            if len(performance_history) < self.min_sample_size:
                return {
                    "passed": False,
                    "reason": f"Amostra insuficiente: {len(performance_history)} < {self.min_sample_size}",
                    "sample_size": len(performance_history)
                }
            
            # Calcular estat√≠sticas
            old_performance = improvement_data.get("old_performance", 0.0)
            new_performance = improvement_data.get("new_performance", 0.0)
            
            # Teste t para uma amostra (comparando com m√©dia hist√≥rica)
            historical_mean = statistics.mean(performance_history)
            historical_std = statistics.stdev(performance_history) if len(performance_history) > 1 else 0.1
            
            # Calcular t-statistic
            n = len(performance_history)
            t_statistic = (new_performance - historical_mean) / (historical_std / math.sqrt(n))
            
            # Graus de liberdade
            df = n - 1
            
            # Valor cr√≠tico para p < 0.05 (aproxima√ß√£o)
            critical_value = 2.0 if df > 30 else 2.5  # Simplificado
            
            # Determinar signific√¢ncia
            is_significant = abs(t_statistic) > critical_value
            p_value_approx = 0.01 if abs(t_statistic) > 3.0 else 0.05 if abs(t_statistic) > 2.0 else 0.1
            
            return {
                "passed": is_significant and p_value_approx < self.significance_level,
                "t_statistic": t_statistic,
                "p_value_approx": p_value_approx,
                "critical_value": critical_value,
                "sample_size": n,
                "historical_mean": historical_mean,
                "historical_std": historical_std,
                "is_significant": is_significant
            }
            
        except Exception as e:
            return {
                "passed": False,
                "error": str(e)
            }
    
    def _validate_practical_significance(self, improvement_data: Dict[str, Any]) -> Dict[str, Any]:
        """Valida signific√¢ncia pr√°tica da melhoria"""
        
        improvement_percentage = improvement_data.get("improvement_percentage", 0.0)
        old_performance = improvement_data.get("old_performance", 0.0)
        new_performance = improvement_data.get("new_performance", 0.0)
        
        # Crit√©rios de signific√¢ncia pr√°tica
        meets_threshold = improvement_percentage >= self.min_improvement_percentage
        positive_improvement = new_performance > old_performance
        meaningful_change = abs(new_performance - old_performance) >= 0.05  # 5% m√≠nimo absoluto
        
        return {
            "passed": meets_threshold and positive_improvement and meaningful_change,
            "improvement_percentage": improvement_percentage,
            "threshold_met": meets_threshold,
            "positive_change": positive_improvement,
            "meaningful_change": meaningful_change,
            "absolute_change": abs(new_performance - old_performance),
            "required_threshold": self.min_improvement_percentage
        }
    
    def _validate_reproducibility(self, agent_id: str, improvement_data: Dict[str, Any]) -> Dict[str, Any]:
        """Valida reprodutibilidade da melhoria"""
        
        try:
            # Obter melhorias recentes do agente
            recent_improvements = self._get_recent_improvements(agent_id, days=7)
            
            if len(recent_improvements) < 2:
                return {
                    "passed": True,  # Primeira melhoria sempre passa
                    "reason": "Primeira melhoria ou dados insuficientes",
                    "improvements_count": len(recent_improvements)
                }
            
            # Analisar consist√™ncia das melhorias
            improvement_percentages = [imp.get("improvement_percentage", 0.0) for imp in recent_improvements]
            current_improvement = improvement_data.get("improvement_percentage", 0.0)
            
            # Calcular variabilidade
            all_improvements = improvement_percentages + [current_improvement]
            mean_improvement = statistics.mean(all_improvements)
            std_improvement = statistics.stdev(all_improvements) if len(all_improvements) > 1 else 0.0
            
            # Coeficiente de varia√ß√£o (CV)
            cv = std_improvement / mean_improvement if mean_improvement > 0 else float('inf')
            
            # Reprodutibilidade baseada em CV baixo
            is_reproducible = cv < 0.5  # CV < 50%
            
            return {
                "passed": is_reproducible,
                "coefficient_of_variation": cv,
                "mean_improvement": mean_improvement,
                "std_improvement": std_improvement,
                "improvements_analyzed": len(all_improvements),
                "is_reproducible": is_reproducible
            }
            
        except Exception as e:
            return {
                "passed": False,
                "error": str(e)
            }
    
    def _validate_consistency(self, agent_id: str, improvement_data: Dict[str, Any]) -> Dict[str, Any]:
        """Valida consist√™ncia da melhoria com padr√µes do agente"""
        
        try:
            # Obter padr√£o de melhorias do agente
            improvement_pattern = self._analyze_improvement_pattern(agent_id)
            
            current_improvement = improvement_data.get("improvement_percentage", 0.0)
            
            if not improvement_pattern:
                return {
                    "passed": True,
                    "reason": "Sem padr√£o hist√≥rico para compara√ß√£o"
                }
            
            # Verificar se est√° dentro do padr√£o esperado
            expected_range = improvement_pattern.get("expected_range", (0.0, 100.0))
            min_expected, max_expected = expected_range
            
            within_pattern = min_expected <= current_improvement <= max_expected
            
            # Verificar tend√™ncia
            trend = improvement_pattern.get("trend", "stable")
            trend_consistent = True
            
            if trend == "improving" and current_improvement < improvement_pattern.get("recent_average", 0.0):
                trend_consistent = False
            elif trend == "declining" and current_improvement > improvement_pattern.get("recent_average", 0.0):
                trend_consistent = False
            
            return {
                "passed": within_pattern and trend_consistent,
                "within_expected_range": within_pattern,
                "trend_consistent": trend_consistent,
                "current_improvement": current_improvement,
                "expected_range": expected_range,
                "agent_trend": trend
            }
            
        except Exception as e:
            return {
                "passed": True,  # Falha na valida√ß√£o n√£o deve bloquear
                "error": str(e)
            }
    
    def _validate_bounds(self, improvement_data: Dict[str, Any]) -> Dict[str, Any]:
        """Valida se os valores est√£o dentro de limites aceit√°veis"""
        
        old_performance = improvement_data.get("old_performance", 0.0)
        new_performance = improvement_data.get("new_performance", 0.0)
        improvement_percentage = improvement_data.get("improvement_percentage", 0.0)
        
        # Verificar limites
        performance_in_bounds = 0.0 <= old_performance <= 1.0 and 0.0 <= new_performance <= 1.0
        improvement_reasonable = -100.0 <= improvement_percentage <= 1000.0  # Limites razo√°veis
        no_negative_performance = old_performance >= 0.0 and new_performance >= 0.0
        
        return {
            "passed": performance_in_bounds and improvement_reasonable and no_negative_performance,
            "performance_in_bounds": performance_in_bounds,
            "improvement_reasonable": improvement_reasonable,
            "no_negative_values": no_negative_performance,
            "old_performance": old_performance,
            "new_performance": new_performance,
            "improvement_percentage": improvement_percentage
        }
    
    def _get_performance_history(self, agent_id: str, days: int = 30) -> List[float]:
        """Obt√©m hist√≥rico de performance de um agente"""
        try:
            if not self.supabase_client:
                return []
            
            end_time = datetime.utcnow()
            start_time = end_time - timedelta(days=days)
            
            result = self.supabase_client.table('performance_metrics').select("current_value").eq('agent_id', agent_id).gte('timestamp', start_time.isoformat()).execute()
            
            return [record['current_value'] for record in result.data if record.get('current_value') is not None]
            
        except Exception as e:
            self.logger.error(f"‚ùå Erro ao obter hist√≥rico: {e}")
            return []
    
    def _get_recent_improvements(self, agent_id: str, days: int = 7) -> List[Dict[str, Any]]:
        """Obt√©m melhorias recentes de um agente"""
        try:
            if not self.supabase_client:
                return []
            
            end_time = datetime.utcnow()
            start_time = end_time - timedelta(days=days)
            
            result = self.supabase_client.table('performance_metrics').select("*").eq('agent_id', agent_id).gte('timestamp', start_time.isoformat()).execute()
            
            improvements = []
            for record in result.data:
                if record.get('improvement_percentage') is not None:
                    improvements.append({
                        "improvement_percentage": record['improvement_percentage'],
                        "timestamp": record['timestamp'],
                        "old_performance": record.get('baseline_value', 0.0),
                        "new_performance": record.get('current_value', 0.0)
                    })
            
            return improvements
            
        except Exception as e:
            self.logger.error(f"‚ùå Erro ao obter melhorias recentes: {e}")
            return []
    
    def _analyze_improvement_pattern(self, agent_id: str) -> Optional[Dict[str, Any]]:
        """Analisa padr√£o de melhorias de um agente"""
        try:
            improvements = self._get_recent_improvements(agent_id, days=30)
            
            if len(improvements) < 3:
                return None
            
            percentages = [imp["improvement_percentage"] for imp in improvements]
            
            # Calcular estat√≠sticas
            mean_improvement = statistics.mean(percentages)
            std_improvement = statistics.stdev(percentages)
            
            # Determinar tend√™ncia
            recent_half = percentages[-len(percentages)//2:]
            older_half = percentages[:len(percentages)//2]
            
            recent_avg = statistics.mean(recent_half)
            older_avg = statistics.mean(older_half)
            
            if recent_avg > older_avg * 1.1:
                trend = "improving"
            elif recent_avg < older_avg * 0.9:
                trend = "declining"
            else:
                trend = "stable"
            
            # Calcular faixa esperada (m√©dia ¬± 2 desvios padr√£o)
            expected_range = (
                max(0.0, mean_improvement - 2 * std_improvement),
                mean_improvement + 2 * std_improvement
            )
            
            return {
                "mean_improvement": mean_improvement,
                "std_improvement": std_improvement,
                "trend": trend,
                "recent_average": recent_avg,
                "expected_range": expected_range,
                "sample_size": len(improvements)
            }
            
        except Exception as e:
            self.logger.error(f"‚ùå Erro ao analisar padr√£o: {e}")
            return None
    
    def _calculate_confidence_score(self, validation_results: Dict[str, Any]) -> float:
        """Calcula score de confian√ßa da valida√ß√£o"""
        
        weights = {
            "statistical": 0.3,
            "practical": 0.25,
            "reproducibility": 0.2,
            "consistency": 0.15,
            "bounds": 0.1
        }
        
        total_score = 0.0
        total_weight = 0.0
        
        for category, result in validation_results.items():
            if category in weights:
                weight = weights[category]
                score = 1.0 if result.get("passed", False) else 0.0
                total_score += score * weight
                total_weight += weight
        
        return total_score / total_weight if total_weight > 0 else 0.0
    
    def _generate_recommendations(self, validation_results: Dict[str, Any]) -> List[str]:
        """Gera recomenda√ß√µes baseadas nos resultados da valida√ß√£o"""
        
        recommendations = []
        
        # Recomenda√ß√µes baseadas em falhas
        if not validation_results.get("statistical", {}).get("passed", True):
            recommendations.append("Aumentar tamanho da amostra para melhor signific√¢ncia estat√≠stica")
        
        if not validation_results.get("practical", {}).get("passed", True):
            recommendations.append("Buscar melhorias mais substanciais para signific√¢ncia pr√°tica")
        
        if not validation_results.get("reproducibility", {}).get("passed", True):
            recommendations.append("Melhorar consist√™ncia das melhorias para maior reprodutibilidade")
        
        if not validation_results.get("consistency", {}).get("passed", True):
            recommendations.append("Alinhar melhorias com padr√£o hist√≥rico do agente")
        
        if not validation_results.get("bounds", {}).get("passed", True):
            recommendations.append("Verificar c√°lculos - valores fora dos limites aceit√°veis")
        
        if not recommendations:
            recommendations.append("Valida√ß√£o aprovada - continuar com melhorias")
        
        return recommendations
    
    def _save_validation_to_suna(self, validation_data: Dict[str, Any]) -> bool:
        """Salva resultado da valida√ß√£o no sistema SUNA"""
        try:
            if not self.supabase_client:
                return False
            
            validation_record = {
                "id": validation_data["validation_id"],
                "agent_id": validation_data["agent_id"],
                "validation_type": "scientific_improvement",
                "overall_passed": validation_data["overall_passed"],
                "confidence_score": validation_data["confidence_score"],
                "validation_criteria": json.dumps(validation_data["validation_results"]),
                "recommendations": json.dumps(validation_data["recommendations"]),
                "timestamp": datetime.utcnow().isoformat(),
                "metadata": json.dumps(validation_data)
            }
            
            result = self.supabase_client.table('validation_results').insert(validation_record).execute()
            
            if result.data:
                self.logger.info(f"‚úÖ Valida√ß√£o salva no SUNA: {validation_data['validation_id']}")
                return True
            else:
                self.logger.error("‚ùå Falha ao salvar valida√ß√£o no SUNA")
                return False
                
        except Exception as e:
            self.logger.error(f"‚ùå Erro ao salvar valida√ß√£o: {e}")
            return False

# Fun√ß√£o de teste para desenvolvimento
def test_validation_system():
    """Teste b√°sico do sistema de valida√ß√£o"""
    print("üéØ Testando Sistema de Valida√ß√£o SUNA-ALSHAM...")
    
    validation_system = ValidationSystem()
    print(f"üî¨ Sistema criado - ID: {validation_system.system_id}")
    
    # Simular dados de melhoria
    improvement_data = {
        "old_performance": 0.65,
        "new_performance": 0.78,
        "improvement_percentage": 20.0,
        "timestamp": datetime.utcnow().isoformat()
    }
    
    # Executar valida√ß√£o
    result = validation_system.validate_improvement("test-agent-id", improvement_data)
    
    if result.get("overall_passed"):
        confidence = result["confidence_score"]
        print(f"‚úÖ Valida√ß√£o APROVADA - Confian√ßa: {confidence:.3f}")
    else:
        print(f"‚ùå Valida√ß√£o REPROVADA: {result.get('error', 'Crit√©rios n√£o atendidos')}")
    
    print("üéâ Teste do Sistema de Valida√ß√£o conclu√≠do!")

if __name__ == "__main__":
    test_validation_system()
