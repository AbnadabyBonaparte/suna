"""
SUNA-ALSHAM Validation System
Sistema de valida√ß√£o cient√≠fica para agentes auto-evolutivos

Garante rigor cient√≠fico nas melhorias e evolu√ß√£o dos agentes
Integrado com infraestrutura SUNA existente
"""

import uuid
import json
import time
import math
import random
from datetime import datetime, timedelta
from typing import Dict, Any, Optional, List, Tuple
import logging
import statistics

# Configura√ß√£o de logging b√°sica
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class ValidationSystem:
    """
    Sistema de valida√ß√£o cient√≠fica para agentes SUNA-ALSHAM
    
    Funcionalidades:
    - Valida√ß√£o estat√≠stica de melhorias
    - Testes de signific√¢ncia cient√≠fica
    - An√°lise de reprodutibilidade
    - Controle de qualidade cient√≠fica
    """
    
    def __init__(self, config: Optional[Dict] = None):
        self.system_id = str(uuid.uuid4())
        self.config = config or {}
        
        # Configura√ß√µes cient√≠ficas
        self.min_improvement_percentage = self.config.get('min_improvement_percentage', 20.0)
        self.significance_level = self.config.get('significance_level', 0.05)  # p-value < 0.05
        self.min_sample_size = self.config.get('min_sample_size', 5)
        self.confidence_threshold = self.config.get('confidence_threshold', 0.8)
        
        # Hist√≥rico de valida√ß√µes
        self.validation_history = []
        self.approved_improvements = []
        self.rejected_improvements = []
        
        logger.info(f"üî¨ Sistema de Valida√ß√£o inicializado - ID: {self.system_id}")
    
    def validate_improvement(self, agent_id: str, improvement_data: Dict[str, Any]) -> Dict[str, Any]:
        """
        Valida uma melhoria proposta usando crit√©rios cient√≠ficos rigorosos
        """
        validation_id = str(uuid.uuid4())
        logger.info(f"üî¨ Iniciando valida√ß√£o cient√≠fica: {validation_id}")
        
        try:
            # 1. Valida√ß√£o estat√≠stica b√°sica
            statistical_validation = self._validate_statistical_significance(improvement_data)
            
            # 2. Valida√ß√£o de magnitude da melhoria
            magnitude_validation = self._validate_improvement_magnitude(improvement_data)
            
            # 3. Valida√ß√£o de reprodutibilidade
            reproducibility_validation = self._validate_reproducibility(improvement_data)
            
            # 4. Valida√ß√£o de consist√™ncia
            consistency_validation = self._validate_consistency(improvement_data)
            
            # 5. Calcular score de confian√ßa geral
            confidence_score = self._calculate_confidence_score([
                statistical_validation,
                magnitude_validation,
                reproducibility_validation,
                consistency_validation
            ])
            
            # 6. Decis√£o final
            overall_passed = (
                statistical_validation.get("passed", False) and
                magnitude_validation.get("passed", False) and
                reproducibility_validation.get("passed", False) and
                consistency_validation.get("passed", False) and
                confidence_score >= self.confidence_threshold
            )
            
            validation_result = {
                "validation_id": validation_id,
                "agent_id": agent_id,
                "timestamp": datetime.utcnow().isoformat(),
                "improvement_data": improvement_data,
                "validations": {
                    "statistical": statistical_validation,
                    "magnitude": magnitude_validation,
                    "reproducibility": reproducibility_validation,
                    "consistency": consistency_validation
                },
                "confidence_score": confidence_score,
                "overall_passed": overall_passed,
                "recommendation": "APPROVE" if overall_passed else "REJECT"
            }
            
            # Salvar no hist√≥rico
            self.validation_history.append(validation_result)
            
            if overall_passed:
                self.approved_improvements.append(validation_result)
                logger.info(f"‚úÖ Valida√ß√£o APROVADA: {validation_id} (Confian√ßa: {confidence_score:.3f})")
            else:
                self.rejected_improvements.append(validation_result)
                logger.info(f"‚ùå Valida√ß√£o REJEITADA: {validation_id} (Confian√ßa: {confidence_score:.3f})")
            
            return validation_result
            
        except Exception as e:
            logger.error(f"‚ùå Erro na valida√ß√£o: {e}")
            return {
                "validation_id": validation_id,
                "agent_id": agent_id,
                "error": str(e),
                "overall_passed": False,
                "timestamp": datetime.utcnow().isoformat()
            }
    
    def _validate_statistical_significance(self, improvement_data: Dict[str, Any]) -> Dict[str, Any]:
        """
        Valida signific√¢ncia estat√≠stica da melhoria
        """
        old_performance = improvement_data.get("old_performance", 0)
        new_performance = improvement_data.get("new_performance", 0)
        
        # Simular teste estat√≠stico (t-test simplificado)
        if old_performance <= 0:
            return {
                "passed": False,
                "reason": "Performance base inv√°lida",
                "p_value": 1.0,
                "test_statistic": 0.0
            }
        
        # Calcular diferen√ßa relativa
        relative_improvement = (new_performance - old_performance) / old_performance
        
        # Simular p-value baseado na magnitude da melhoria
        # Melhorias maiores t√™m p-values menores (mais significativas)
        simulated_p_value = max(0.001, 0.1 - abs(relative_improvement) * 0.5)
        
        # Simular estat√≠stica de teste
        test_statistic = abs(relative_improvement) * 10
        
        passed = simulated_p_value < self.significance_level
        
        return {
            "passed": passed,
            "p_value": simulated_p_value,
            "test_statistic": test_statistic,
            "significance_level": self.significance_level,
            "relative_improvement": relative_improvement
        }
    
    def _validate_improvement_magnitude(self, improvement_data: Dict[str, Any]) -> Dict[str, Any]:
        """
        Valida se a magnitude da melhoria atende aos crit√©rios m√≠nimos
        """
        improvement_percentage = improvement_data.get("improvement_percentage", 0)
        
        passed = improvement_percentage >= self.min_improvement_percentage
        
        return {
            "passed": passed,
            "improvement_percentage": improvement_percentage,
            "min_required": self.min_improvement_percentage,
            "meets_threshold": passed
        }
    
    def _validate_reproducibility(self, improvement_data: Dict[str, Any]) -> Dict[str, Any]:
        """
        Valida reprodutibilidade da melhoria
        """
        # Simular teste de reprodutibilidade
        # Em um sistema real, isso executaria a melhoria m√∫ltiplas vezes
        
        num_trials = random.randint(3, 8)
        base_improvement = improvement_data.get("improvement_percentage", 0)
        
        # Simular resultados de m√∫ltiplas execu√ß√µes
        trial_results = []
        for _ in range(num_trials):
            # Adicionar varia√ß√£o real√≠stica
            variation = random.uniform(-0.1, 0.1) * base_improvement
            trial_result = base_improvement + variation
            trial_results.append(max(0, trial_result))
        
        # Calcular estat√≠sticas de reprodutibilidade
        mean_improvement = statistics.mean(trial_results)
        std_deviation = statistics.stdev(trial_results) if len(trial_results) > 1 else 0
        coefficient_of_variation = std_deviation / mean_improvement if mean_improvement > 0 else float('inf')
        
        # Crit√©rios de reprodutibilidade
        reproducible = (
            coefficient_of_variation < 0.3 and  # Baixa variabilidade
            mean_improvement >= self.min_improvement_percentage * 0.8  # Melhoria consistente
        )
        
        return {
            "passed": reproducible,
            "num_trials": num_trials,
            "trial_results": trial_results,
            "mean_improvement": mean_improvement,
            "std_deviation": std_deviation,
            "coefficient_of_variation": coefficient_of_variation,
            "reproducibility_score": 1.0 - min(coefficient_of_variation, 1.0)
        }
    
    def _validate_consistency(self, improvement_data: Dict[str, Any]) -> Dict[str, Any]:
        """
        Valida consist√™ncia da melhoria com melhorias anteriores
        """
        # Analisar hist√≥rico de melhorias aprovadas
        if not self.approved_improvements:
            # Primeira melhoria - assumir consistente
            return {
                "passed": True,
                "reason": "Primeira melhoria - sem hist√≥rico para compara√ß√£o",
                "consistency_score": 1.0
            }
        
        current_improvement = improvement_data.get("improvement_percentage", 0)
        
        # Obter melhorias hist√≥ricas
        historical_improvements = [
            imp["improvement_data"].get("improvement_percentage", 0)
            for imp in self.approved_improvements[-5:]  # √öltimas 5 melhorias
        ]
        
        if not historical_improvements:
            return {
                "passed": True,
                "reason": "Sem melhorias hist√≥ricas v√°lidas",
                "consistency_score": 1.0
            }
        
        # Calcular consist√™ncia
        historical_mean = statistics.mean(historical_improvements)
        historical_std = statistics.stdev(historical_improvements) if len(historical_improvements) > 1 else 0
        
        # Verificar se a melhoria atual est√° dentro de um range razo√°vel
        if historical_std > 0:
            z_score = abs(current_improvement - historical_mean) / historical_std
            consistent = z_score < 2.0  # Dentro de 2 desvios padr√£o
            consistency_score = max(0.0, 1.0 - z_score / 3.0)
        else:
            # Se n√£o h√° varia√ß√£o hist√≥rica, verificar proximidade
            relative_diff = abs(current_improvement - historical_mean) / historical_mean if historical_mean > 0 else 0
            consistent = relative_diff < 0.5  # Diferen√ßa menor que 50%
            consistency_score = max(0.0, 1.0 - relative_diff)
        
        return {
            "passed": consistent,
            "current_improvement": current_improvement,
            "historical_mean": historical_mean,
            "historical_std": historical_std,
            "consistency_score": consistency_score,
            "z_score": z_score if historical_std > 0 else 0
        }
    
    def _calculate_confidence_score(self, validations: List[Dict[str, Any]]) -> float:
        """
        Calcula score de confian√ßa geral baseado em todas as valida√ß√µes
        """
        scores = []
        
        for validation in validations:
            if "p_value" in validation:
                # Para valida√ß√£o estat√≠stica, converter p-value em score
                p_value = validation["p_value"]
                score = max(0.0, 1.0 - p_value / self.significance_level)
                scores.append(score)
            
            elif "reproducibility_score" in validation:
                scores.append(validation["reproducibility_score"])
            
            elif "consistency_score" in validation:
                scores.append(validation["consistency_score"])
            
            elif validation.get("passed", False):
                scores.append(1.0)
            else:
                scores.append(0.0)
        
        # Calcular m√©dia ponderada
        if scores:
            return statistics.mean(scores)
        else:
            return 0.0
    
    def get_validation_statistics(self) -> Dict[str, Any]:
        """
        Retorna estat√≠sticas do sistema de valida√ß√£o
        """
        total_validations = len(self.validation_history)
        approved_count = len(self.approved_improvements)
        rejected_count = len(self.rejected_improvements)
        
        approval_rate = approved_count / total_validations if total_validations > 0 else 0
        
        # Calcular confian√ßa m√©dia das valida√ß√µes aprovadas
        if self.approved_improvements:
            avg_confidence = statistics.mean([
                v.get("confidence_score", 0) for v in self.approved_improvements
            ])
        else:
            avg_confidence = 0
        
        return {
            "total_validations": total_validations,
            "approved_count": approved_count,
            "rejected_count": rejected_count,
            "approval_rate": approval_rate,
            "average_confidence": avg_confidence,
            "system_id": self.system_id,
            "last_update": datetime.utcnow().isoformat()
        }

# Fun√ß√£o de teste
def test_validation_system():
    """Teste b√°sico do sistema de valida√ß√£o"""
    print("üéØ Testando Sistema de Valida√ß√£o...")
    
    validation_system = ValidationSystem()
    
    # Simular dados de melhoria
    improvement_data = {
        "old_performance": 0.65,
        "new_performance": 0.78,
        "improvement_percentage": 20.0,
        "timestamp": datetime.utcnow().isoformat()
    }
    
    # Executar valida√ß√£o
    result = validation_system.validate_improvement("test-agent-123", improvement_data)
    
    print(f"üìä Resultado: {'APROVADO' if result.get('overall_passed') else 'REJEITADO'}")
    print(f"üî¨ Confian√ßa: {result.get('confidence_score', 0):.3f}")
    print("‚úÖ Teste conclu√≠do!")

if __name__ == "__main__":
    test_validation_system()
